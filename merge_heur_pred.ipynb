{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`NOTE:` Merge cells function in heuristics is not correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "SCITSR_PATH = '/Users/longhoang/Developer/table-recognition/data/SciTSR-partition'\n",
    "MODEL_WEIGHT = '/Users/longhoang/Developer/table-recognition/pret-models/split0.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "from data_utils.utils import *\n",
    "from merge.heuristics import *\n",
    "from dataset.dataset import ImageDataset\n",
    "from modules.split_modules import SplitModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cpu'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# LOAD MODEL\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "net = SplitModel(3)\n",
    "net = torch.nn.DataParallel(net).to(device)\n",
    "\n",
    "if device == 'cuda':\n",
    "    net.load_state_dict(torch.load(MODEL_WEIGHT))\n",
    "else:\n",
    "    net.load_state_dict(torch.load(MODEL_WEIGHT, map_location=torch.device('cpu')))\n",
    "\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Got 10000 images\n",
      "- Loaded 10000 labels for Merge module\n",
      "- Loaded texts positions for 10000 images\n",
      "- Loaded dataset with 10000 examples\n"
     ]
    }
   ],
   "source": [
    "# Load images\n",
    "img_dir = os.path.join(SCITSR_PATH, 'train', 'img')\n",
    "imgs_paths = [os.path.join(img_dir, p) for p in os.listdir(img_dir)]\n",
    "print(f'- Got {len(imgs_paths)} images')\n",
    "\n",
    "# Load Merge labels\n",
    "merge_json = os.path.join(SCITSR_PATH, 'train', 'label', 'merge_label.json')\n",
    "merge_labels = load_json(merge_json)\n",
    "print(f\"- Loaded {len(merge_labels)} labels for Merge module\")\n",
    "\n",
    "# Load Text posisions\n",
    "chunk_json = os.path.join(SCITSR_PATH, 'train', 'label', 'chunk_label.json')\n",
    "chunk_labels = load_json(chunk_json)\n",
    "print(f\"- Loaded texts positions for {len(chunk_labels)} images\")\n",
    "\n",
    "# Load dataset\n",
    "split_json = os.path.join(SCITSR_PATH, 'train', 'label', 'split_label.json')\n",
    "split_labels = load_json(split_json)\n",
    "dataset = ImageDataset(img_dir, split_labels, 8, scale=1, min_width=10, returns_image_name=True)\n",
    "print(f'- Loaded dataset with {len(dataset)} examples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "IDX = 20\n",
    "img, label, img_name = dataset[IDX]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_heur_pred(model: SplitModel, \n",
    "                    img_dir: str, \n",
    "                    split_json: str, \n",
    "                    merge_json: str, \n",
    "                    chunk_json: str,\n",
    "                    iou_th: float = 0.7):\n",
    "    '''\n",
    "    Args:\n",
    "    model -- Split model\n",
    "    img_dir -- string, path to image folder of on of train, val, or test set\n",
    "    split_json -- string, path to json ground truth file for Split module\n",
    "    merge_json -- string, path to json ground truth file for Merge module\n",
    "    chunk_json -- string, path to json file that contain chunk info (coordinates of texts)\n",
    "    iou_th -- IoU threshold\n",
    "    '''\n",
    "    # Load images\n",
    "    imgs_paths = [os.path.join(img_dir, p) for p in os.listdir(img_dir)]\n",
    "    print(f'- Got {len(imgs_paths)} images')\n",
    "\n",
    "    # Load Merge labels\n",
    "    merge_labels = load_json(merge_json)\n",
    "    print(f\"- Loaded {len(merge_labels)} labels for Merge module\")\n",
    "\n",
    "    # Load Text posisions\n",
    "    chunk_labels = load_json(chunk_json)\n",
    "    print(f\"- Loaded texts positions for {len(chunk_labels)} images\")\n",
    "\n",
    "    # Load dataset\n",
    "    split_labels = load_json(split_json)\n",
    "    dataset = ImageDataset(img_dir, split_labels, 8, scale=1, min_width=10, returns_image_name=True)\n",
    "    print(f'- Loaded dataset with {len(dataset)} examples')\n",
    "\n",
    "    single_col_or_row = []\n",
    "    shape_mismatch = []\n",
    "    wrong_label = []\n",
    "    wrong_coordinates = []\n",
    "    f1s, recalls, precisions = [], [], []\n",
    "    model.eval()\n",
    "\n",
    "    for img, label, img_name in tqdm(dataset):\n",
    "        texts_pos = chunk_labels[img_name]\n",
    "        \n",
    "        # load ground truth\n",
    "        r_gt, c_gt, R_gt, D_gt = load_merge_gt(merge_labels, img_name)\n",
    "        if R_gt.ndim != 2 or D_gt.ndim != 2: \n",
    "            single_col_or_row.append(img_name)\n",
    "            continue\n",
    "        row_gt_idxs, col_gt_idxs = borders(r_gt), borders(c_gt)\n",
    "        cells_gt = get_cells(row_gt_idxs, col_gt_idxs)\n",
    "        if len(cells_gt) != R_gt.shape[0] * D_gt.shape[1]:\n",
    "            shape_mismatch.append(img_name)\n",
    "            continue\n",
    "        cells_merged_gt = merge_cells(cells_gt, R_gt, D_gt)\n",
    "        \n",
    "        # get predictions\n",
    "        with torch.no_grad():\n",
    "            r_pred, c_pred = model(img.unsqueeze(0))\n",
    "        r_pred, c_pred = process_split_results(r_pred, c_pred)\n",
    "        r_pred, c_pred = refine_split_results(r_pred), refine_split_results(c_pred)\n",
    "        row_pred_idxs, col_pred_idxs = borders(r_pred), borders(c_pred)                                      \n",
    "        cells_pred = get_cells(row_pred_idxs, col_pred_idxs)\n",
    "        if len(row_pred_idxs) == 0 or len(col_pred_idxs) == 0:\n",
    "            wrong_label.append(img_name)\n",
    "            continue\n",
    "        \n",
    "        # apply Merge heuristics\n",
    "        R_pred, D_pred = create_pred_matrices(row_pred_idxs, col_pred_idxs)\n",
    "        rule1(cells_pred, texts_pos, R_pred, D_pred)\n",
    "        rule2(cells_pred, texts_pos, R_pred, D_pred)\n",
    "        cells_merged_pred = merge_cells(cells_pred, R_pred, D_pred)\n",
    "        \n",
    "        # get evaluation scores\n",
    "        f1, rec, prec, name = eval(cells_merged_pred, cells_merged_gt, threshold=iou_th, img_name=img_name)\n",
    "        if name: \n",
    "            wrong_coordinates.append(name)\n",
    "        f1s.append(f1); recalls.append(rec); precisions.append(prec)\n",
    "\n",
    "    f1_avg, rec_avg, prec_avg = np.mean(f1s), np.mean(recalls), np.mean(precisions)\n",
    "    print(f'F1: {f1_avg:.4f} ; Recall: {rec_avg:.4f} ; Precision: {prec_avg:.4f}')\n",
    "    scores = {\n",
    "        'f1': f1_avg, \n",
    "        'recall': rec_avg, \n",
    "        'precision': prec_avg\n",
    "    }\n",
    "    errors = {\n",
    "        'single': single_col_or_row, \n",
    "        'shape_mismatch': shape_mismatch, \n",
    "        'wrong_label': wrong_label,\n",
    "        'wrong_coordinates': wrong_coordinates\n",
    "    }\n",
    "    return scores, errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on train set\n",
    "def evaluate_set(set_dir, **kwargs):\n",
    "    img_dir = os.path.join(set_dir, 'img')\n",
    "    label_dir = os.path.join(set_dir, 'label')\n",
    "    split_json = os.path.join(label_dir, 'split_label.json')\n",
    "    merge_json = os.path.join(label_dir, 'merge_label.json')\n",
    "    chunk_json = os.path.join(label_dir, 'chunk_label.json')\n",
    "    return merge_heur_pred(net, img_dir, split_json, merge_json, chunk_json, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Got 10000 images\n",
      "- Loaded 10000 labels for Merge module\n",
      "- Loaded texts positions for 10000 images\n",
      "- Loaded dataset with 10000 examples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▏      | 3201/10000 [16:21<34:45,  3.26it/s]  \n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "tuple index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m train_dir \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(SCITSR_PATH, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m scores, errors \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate_set\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miou_th\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.6\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[10], line 8\u001b[0m, in \u001b[0;36mevaluate_set\u001b[0;34m(set_dir, **kwargs)\u001b[0m\n\u001b[1;32m      6\u001b[0m merge_json \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(label_dir, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmerge_label.json\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      7\u001b[0m chunk_json \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(label_dir, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mchunk_label.json\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 8\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmerge_heur_pred\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnet\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimg_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msplit_json\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmerge_json\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunk_json\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[8], line 50\u001b[0m, in \u001b[0;36mmerge_heur_pred\u001b[0;34m(model, img_dir, split_json, merge_json, chunk_json, iou_th)\u001b[0m\n\u001b[1;32m     48\u001b[0m row_gt_idxs, col_gt_idxs \u001b[38;5;241m=\u001b[39m borders(r_gt), borders(c_gt)\n\u001b[1;32m     49\u001b[0m cells_gt \u001b[38;5;241m=\u001b[39m get_cells(row_gt_idxs, col_gt_idxs)\n\u001b[0;32m---> 50\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(cells_gt) \u001b[38;5;241m!=\u001b[39m R_gt\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m*\u001b[39m \u001b[43mD_gt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m:\n\u001b[1;32m     51\u001b[0m     shape_mismatch\u001b[38;5;241m.\u001b[39mappend(img_name)\n\u001b[1;32m     52\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "\u001b[0;31mIndexError\u001b[0m: tuple index out of range"
     ]
    }
   ],
   "source": [
    "train_dir = os.path.join(SCITSR_PATH, 'train')\n",
    "scores, errors = evaluate_set(train_dir, iou_th=0.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8, 1057, 10)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(errors['single']), len(errors['shape_mismatch']), len(errors['wrong_label'], len(errors['wrong_coordinates']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_dict(dict, dir_path):\n",
    "    os.makedirs(dir_path, exist_ok=True)\n",
    "    for key in errors.keys():\n",
    "        p = os.path.join(dir_path, key + '.json')\n",
    "        with open(p, 'w') as f:\n",
    "            json.dump(dict[key], f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ERROR_PATH = '/Users/longhoang/Developer/table-reg/code/deep-split-merge-scitsr/merge/error/train'\n",
    "save_dict(errors, ERROR_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got 1971 images\n",
      "Loaded 1971 labels for Merge module\n",
      "Loaded texts positions for 11971 images\n",
      "Loaded dataset with 1971 examples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1971/1971 [09:37<00:00,  3.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1: 0.7573 ; Recall: 0.7973 ; Precision: 0.7319\n"
     ]
    }
   ],
   "source": [
    "VAL_DIR = '/Users/longhoang/Developer/table-reg/data/scitsr-split-train/val'\n",
    "val_scores, val_errors = evaluate_set(VAL_DIR, iou_th=0.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VAL_ERROR_PATH = '/Users/longhoang/Developer/table-reg/code/deep-split-merge-scitsr/merge/error/val'\n",
    "save_dict(val_errors, VAL_ERROR_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 229, 2)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(val_errors['single']), len(val_errors['shape_mismatch']), len(val_errors['wrong_label'], len(val_errors['wrong_coordinates']))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "longhoang",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
